#!/bin/bash -x
source /usr/lib/hustler/bin/qubole-bash-lib.sh

BASEDIR="$( cd -P "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
AIRFLOW_HOME=$(echo $AIRFLOW_HOME)
AIRFLOW_SCHEDULER_PID=${AIRFLOW_HOME}/scheduler.pid
AIRFLOW_WEBSERVER_PID=${AIRFLOW_HOME}/webserver.pid
AIRFLOW_PARENT_WEBSERVER_PID=${AIRFLOW_HOME}/parent_webserver.pid
AIRFLOW_RABBITMQ_PID=${AIRFLOW_HOME}/rabbitmq.pid
AIRFLOW_WORKER_PID=${AIRFLOW_HOME}/worker.pid
AIRFLOW_LOG_DIR=/media/ephemeral0/logs/airflow/
CLUSTER_ID=$(echo $CLUSTER_ID)
qubole_base_url=$(echo $QUBOLE_BASE_URL)
use_celery_airflow=$(echo $USE_CELERY_AIRFLOW)
use_cluster_broker_airflow=$(echo $USE_CLUSTER_BROKER_AIRFLOW)
use_cluster_datastore=$(echo $USE_CLUSTER_DATASTORE)
worker_child_pids=

function start_all() {

activate_virtualenv
setup_default_datastore
start_rabbitmq
initialize_database
deactivate_virtualenv
start_airflow_processes
}

function setup_default_datastore() {

# Setup default datatstore only if user is using cluster datastore and has not provided his own datatstore
if [[ "$use_cluster_datastore" == "True" ]]; then
    echo "Setting up default datastore"
    start_postgres_server
    sudo -u postgres psql -c "CREATE USER root WITH PASSWORD '${CLUSTER_ID}';" &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
    sudo -u postgres psql -c "CREATE DATABASE airflow;" &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
    sudo -u postgres psql -c "GRANT ALL PRIVILEGES ON DATABASE airflow TO root;" &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
fi
}

function start_rabbitmq() {
# Start only if user is using celery executor and has not provided his own broker url
if [[ "$use_cluster_broker_airflow" == "True" ]]; then
    nohup /sbin/service rabbitmq-server start &> ${AIRFLOW_LOG_DIR}/rabbitmq-server.log

    # Location of pid file generated by rabbitmq on bringup
    pid=$(cat /var/run/rabbitmq/pid)

    if [[ ! -z $pid ]]; then
        echo "Rabbitmq startup successful"
        # Fetching child process id which is the correct process to watch for. If child dies then all rabbitmq services
        # goes down, if $pid goes down, then $cpid still stays intact & init-process becomes parent.
        cpid=$(echo $(pgrep -P $pid))
        echo "$cpid" > ${AIRFLOW_RABBITMQ_PID}
    else
        echo "Rabbitmq startup failed, could not fetch process id"
    fi

    rabbitmq-plugins enable rabbitmq_management # enable rabbitmq management plugin to view dashboard
fi
}

function activate_virtualenv(){
    source ${BASEDIR}/scripts/virtualenv.sh activate
}

function deactivate_virtualenv(){
    source ${BASEDIR}/scripts/virtualenv.sh deactivate
}

function initialize_database(){
  nohup airflow db init &>> ${AIRFLOW_LOG_DIR}/database-init.log
}

function stop_worker() {

# Kill the main worker process, when used by monit it will not exist, in any case good to clean it up
kill_process ${AIRFLOW_WORKER_PID}

# Kill the child processes, using grep technique because it could be possible that parent process do not exist
worker_child_pids=$(ps -ef | egrep  "celeryd(.)*Worker-[0-9]+" | awk '{ print $2 }')

if [[ ! -z $worker_child_pids ]]; then
    kill $worker_child_pids
fi

}

function kill_process() {
    kill -9 $(cat $1)
}

function start_postgres_server() {
    if [[ $PROVIDER == aws ]] && [[ $IS_AL2_AMI != "true" ]]; then
        sudo service postgresql initdb &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
        sed -i "/host \+all/d" /var/lib/pgsql9/data/pg_hba.conf
        echo -e "host            all         all         all         password" >> /var/lib/pgsql9/data/pg_hba.conf
        sudo service postgresql start &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
    else
        sudo postgresql-setup initdb &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
        sed -i "/host \+all/d" /var/lib/pgsql/data/pg_hba.conf
        echo -e "host            all         all         all         password" >> /var/lib/pgsql/data/pg_hba.conf
        nohup systemctl start postgresql &>> ${AIRFLOW_LOG_DIR}/datastore-init.log
     fi
}

function start_airflow_processes() {
    if [[ `nodeinfo quboled_env_python_version` == 3.5* ]]; then
      sudo chown -R yarn:yarn ${AIRFLOW_VIRTUALENV_LOC}/lib/python3.5/site-packages/
   elif [[ `nodeinfo quboled_env_python_version` == 3.7* ]]; then
      sub_string="e-"
      base_env="${AIRFLOW_VIRTUALENV_LOC//$sub_string/}"
      sudo chown -R yarn:yarn ${AIRFLOW_VIRTUALENV_LOC}/*
      sudo chown -R yarn:yarn ${base_env}/*
   fi
   sudo su - airflow -c"${BASEDIR}/scripts/start_airflow_processes.sh ${SERVICE}"
}

function start(){

case "${SERVICE}" in
  "all" )
    start_all
    ;;
  "rabbitmq" )
    start_rabbitmq
    ;;
  * )
    start_airflow_processes
    ;;
esac

}

function stop(){

case "${SERVICE}" in
  "scheduler" )
    kill -9 $(pgrep -P  $(cat ${AIRFLOW_SCHEDULER_PID}))
    kill_process ${AIRFLOW_SCHEDULER_PID}
    ;;
  "webserver" )
    pgrep -f gunicorn | xargs sudo kill -9
    kill_process ${AIRFLOW_PARENT_WEBSERVER_PID}
    ;;
  "rabbitmq" )
    kill_process ${AIRFLOW_RABBITMQ_PID}
    ;;
  "worker" )
    stop_worker
    ;;
  * )

    echo "only all, scheduler and webserver are supported"
    exit 1
    ;;
esac

}

SERVICE=$2

case "$1" in
  "start" )
    start
    ;;
  "stop" )
    stop
    ;;
  "restart" )
    stop
    start
    ;;
  * )
    echo "only start, stop and restart are supported"
    exit 1
    ;;
esac
